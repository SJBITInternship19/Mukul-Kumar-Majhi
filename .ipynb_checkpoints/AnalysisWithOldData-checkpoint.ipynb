{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-24a8de8d2887>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#importing necessary modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gully.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "#importing necessary modules\n",
    "import pandas as pd\n",
    "tweets =pd.read_csv(\"gully.csv\",header=0)\n",
    "tweets.head()\n",
    "\n",
    "\n",
    "tweets_text = pd.DataFrame(tweets)\n",
    "# print (len(tweets_text))\n",
    "\n",
    "#cleaning up tweets\n",
    "import re\n",
    "\n",
    "def CleanTweet(tweet):\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"').replace(\"|\",\"\")\n",
    "    #replace non ascii characters\n",
    "    re.sub(r'[^\\\\x00-\\\\xff]',r'', tweet) \n",
    "    tweet = tweet.replace(\"\\\\xe2\",\"\").replace(\"\\\\x80\",\"\").replace(\"\\\\x99\",\"\").replace(\"\\\\xf0\",\"\").replace(\"\\\\x9f\",\"\").replace(\"\\\\x98\",\"\").replace(\"\\\\xad\",\"\").replace(\"\\\\xa6\",\"\").replace(\"\\\\x9f\",\"\")\n",
    "    return tweet\n",
    "\n",
    "text = tweets_text['text'].values.tolist()\n",
    "# len(text)\n",
    "clean_tweets = []\n",
    "for i in text:\n",
    "    clean_tweets.append(CleanTweet(i))\n",
    "    \n",
    "    \n",
    "\n",
    "#Process tweets for feature extraction\n",
    "#tokenization and normalization\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from nltk.corpus import stopwords \n",
    "#tokenization\n",
    "def process(tweet, tokenizer=TweetTokenizer(), stopwords=[]): \n",
    "  tokens = tokenizer.tokenize(tweet) \n",
    "  return [tok for tok in tokens if tok not in stopwords and not  \n",
    "          tok.isdigit()]\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer() \n",
    "punct = list(string.punctuation) \n",
    "stopword_list = stopwords.words('english') + punct + ['rt', 'via', '...', 'AT_USER', 'URL', \"'\"]\n",
    "#normalization         \n",
    "def normalize_contractions(tokens, stopwords =[]): \n",
    "  token_map = { \n",
    "    \"i'm\": \"i am\", \n",
    "    \"you're\": \"you are\", \n",
    "    \"it's\": \"it is\", \n",
    "    \"we're\": \"we are\", \n",
    "    \"we'll\": \"we will\",\n",
    "    \"ain't\": \"are not\",\n",
    "    \"ive\" : \"i have\",\n",
    "    \"aint\": \"are not\"\n",
    "      } \n",
    "  for tok in tokens: \n",
    "    if tok in token_map.keys(): \n",
    "      for item in token_map[tok].split():\n",
    "        if item not in stopwords:\n",
    "         yield item\n",
    "    else: \n",
    "      yield tok\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "def getFeatureVector(tweet):\n",
    "        featureVector = []\n",
    "        stopword_list = stopwords.words('english') + punct + ['rt', 'via', '...', 'AT_USER', 'URL', 'https', 'https',\"'\"]\n",
    "        words = list(normalize_contractions(process(tweet, tokenizer=tweet_tokenizer, stopwords= stopword_list), stopwords= stopword_list))\n",
    "        for w in words:\n",
    "        #replace two or more with two occurrences\n",
    "            w = replaceTwoOrMore(w)  \n",
    "            #check if the word starts with an alphabet\n",
    "            val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "            if val is None:\n",
    "                continue\n",
    "            else:    \n",
    "                featureVector.append(w.lower())\n",
    "        return featureVector\n",
    "\n",
    " #importting labelled data\n",
    "import csv\n",
    "import nltk\n",
    "inpTweets = csv.reader(open('final.txt', 'r'), delimiter='\\t', quotechar='|')\n",
    "labeled_tweets = []\n",
    "for row in inpTweets:\n",
    "    sentiment = row[1]\n",
    "    tweet = row[0]\n",
    "    TrainedClean = CleanTweet(tweet)\n",
    "    featureVector = getFeatureVector(TrainedClean)\n",
    "    labeled_tweets.append((featureVector, sentiment))\n",
    "\n",
    "#pulling only the words for vectorization\n",
    "def words_in_tweets(labeled_tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in labeled_tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "word_features = words_in_tweets(labeled_tweets)\n",
    "\n",
    "#rejoining the words with sentimets for bag of words\n",
    "def JoinWords(words):\n",
    "    return( ' '.join(item for item in words))\n",
    "\n",
    "labeled_data= pd.DataFrame((labeled_tweets), columns = ('Vectors', 'label'), copy = True)\n",
    "\n",
    "labeled_data['Vectors']=labeled_data['Vectors'].apply(func = JoinWords)\n",
    "\n",
    "X = labeled_data['Vectors']\n",
    "y = labeled_data['label']\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "#vectorization\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = 'word', preprocessor = None, tokenizer = None, stop_words = stopword_list, lowercase = False)\n",
    "\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "# print (len(vocab))\n",
    "# print (len(word_features))\n",
    "# X_train_dtm\n",
    "    \n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "\n",
    "# import and initiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "%time nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "from sklearn import metrics\n",
    "nba = metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "\n",
    "# import and initiate a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression()\n",
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)\n",
    "#make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "# calculate accuracy\n",
    "# metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "from sklearn import metrics\n",
    "lra = metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "#Initialize the classifier\n",
    "from sklearn import svm\n",
    "supportvec = svm.LinearSVC()\n",
    "# train the model using X_train_dtm\n",
    "%time supportvec.fit(X_train_dtm, y_train)\n",
    "#make class predictions for X_test_dtm\n",
    "y_pred_class = supportvec.predict(X_test_dtm)\n",
    "# calculate accuracy\n",
    "svma = metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "x=['nba','lra','svma']\n",
    "y=[nba*100,lra*100,svma*100]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(x,y)\n",
    "plt.savefig('MLModelComp.png', dpi=1200)\n",
    "\n",
    "\n",
    "final_tweets = [CleanTweet(i) for i in clean_tweets]\n",
    "#deduplication\n",
    "final_tweets = [final_tweets[i] for i in range(len(final_tweets)) if i==0 or final_tweets[i] != final_tweets[i-1]]\n",
    "\n",
    "#Process, clean and extract features to prepare for classifier \n",
    "processed_tweets = [] \n",
    "for item in final_tweets:\n",
    "    featureVector = getFeatureVector(item)\n",
    "    joined_strings = JoinWords(featureVector)\n",
    "    processed_tweets.append(joined_strings)\n",
    "\n",
    "collected_tweets = pd.DataFrame(processed_tweets)\n",
    "collected_tweets.head()\n",
    "final_collected_tweets = collected_tweets[0]\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer = 'word', preprocessor = None, tokenizer = None, stop_words = stopword_list,  binary = True, lowercase = False)\n",
    "\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "X_test_dtm = vectorizer.transform(final_collected_tweets)\n",
    "\n",
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "\n",
    "# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "%time nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# sorting the classified data based on derived sentiments\n",
    "classifiedData = [[final_tweets[x][1:],y_pred_class[x]] for x in range(len(final_tweets))]\n",
    "filtered_sentenceall = [getFeatureVector(classifiedData[i][0]) for i in range(len(classifiedData))]\n",
    "\n",
    "classifiedDatap = [[final_tweets[x][1:],y_pred_class[x]] for x in range(len(final_tweets)) if y_pred_class[x] == '1']\n",
    "filtered_sentencep = [getFeatureVector(classifiedDatap[i][0]) for i in range(len(classifiedDatap))]\n",
    "\n",
    "classifiedDatan = [[final_tweets[x][1:],y_pred_class[x]] for x in range(len(final_tweets)) if y_pred_class[x] == '0']\n",
    "filtered_sentencen = [getFeatureVector(classifiedDatan[i][0]) for i in range(len(classifiedDatan))]\n",
    "\n",
    "filtered_sentencepn=[]\n",
    "\n",
    "for i in range(len(filtered_sentenceall)):\n",
    "    for j in filtered_sentenceall[i]:\n",
    "        filtered_sentencepn.append(j)\n",
    "        \n",
    "filtered_sentencepos=[]\n",
    "\n",
    "for i in range(len(filtered_sentencep)):\n",
    "    for j in filtered_sentencep[i]:\n",
    "        filtered_sentencepos.append(j)\n",
    "\n",
    "filtered_sentenceneg=[]\n",
    "\n",
    "for i in range(len(filtered_sentencen)):\n",
    "    for j in filtered_sentencen[i]:\n",
    "        filtered_sentenceneg.append(j)\n",
    "\n",
    "customfilters=['x8d','xa4','xef','xb8','xa5','x94','x9d','x8f','gully','boy','trailer','ranveer','singh','alia','bhatt','zoya','akthar','launch','official','video','url','liked','akhtar']\n",
    "\n",
    "filtered_sentence2all = [i for i in  filtered_sentencepn if i not in customfilters]\n",
    "filtered_sentence2pos = [i for i in  filtered_sentencepos if i not in customfilters]\n",
    "\n",
    "# initiate wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt1\n",
    "\n",
    "\n",
    "wordcloud1 = WordCloud(background_color='black', max_words = 1000,  max_font_size = 60).generate(' '.join(filtered_sentence2all))\n",
    "print(wordcloud1)\n",
    "fig1 = plt1.figure(1)\n",
    "plt1.imshow(wordcloud1)\n",
    "plt1.axis('off')\n",
    "plt1.show()\n",
    "fig1.savefig(\"word1.png\", dpi=1200)\n",
    "\n",
    "wordcloud2 = WordCloud(background_color='black', max_words = 1000,  max_font_size = 60).generate(' '.join(filtered_sentence2pos))\n",
    "print(wordcloud2)\n",
    "fig2 = plt1.figure(1)\n",
    "plt1.imshow(wordcloud2)\n",
    "plt1.axis('off')\n",
    "plt1.show()\n",
    "fig2.savefig(\"word2.png\", dpi=1200)\n",
    "\n",
    "# wc(filtered_sentence2all,'black','AllML' )\n",
    "# wc(filtered_sentence2pos,'black','posML' )\n",
    "\n",
    "#extract counts for piechart\n",
    "pos = len([i for i,j in classifiedData if j=='1'])\n",
    "neg =  len([i for i,j in classifiedData if j=='0'])/2\n",
    "\n",
    "labels = 'Negative',' Postive'\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie([neg,pos], labels=['neg','pos'],colors=['r','g'],\n",
    "        shadow=True, startangle=90)\n",
    "plt.title=('Sentiment Analysis')\n",
    "#plt.plot()\n",
    "plt.savefig('pieML.png', dpi=900)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
